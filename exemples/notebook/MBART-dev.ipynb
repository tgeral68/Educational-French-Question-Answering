{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a403c731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/people/gerald/Documents/repositories/Educational-French-Question-Answering\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd94971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import evaluate\n",
    "import random\n",
    "from src.data_utils.pb_corpus import FQAGPBDataset\n",
    "from src.data_utils.corpus import MixedDataset, KeyMapDataset\n",
    "from src.model.mbart_qg import MBARTQGDataLoaderCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c049089",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'\n",
    "os.environ['QA_LOG'] = \"/data/workdir/gerald/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee4ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "datasets_name = [\"squad-en-en.pb.json\",\"fquad-fr-fr.pb.json\"]\n",
    "datasets = {}\n",
    "\n",
    "split = \"train\"\n",
    "\n",
    "for dataset_name in datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        data = json.load(f)\n",
    "        datasets[dataset_name.split('.')[0]] = FQAGPBDataset(data['train'], sampler = lambda x : [x[random.randint(0, len(x) - 1)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda9181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = KeyMapDataset(MixedDataset(*datasets.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7beab71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_default': True,\n",
       " 'id': 0,\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to <hl>Saint Bernadette Soubirous<hl> in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question_type': 'NONE',\n",
       " 'input_lang': 250008,\n",
       " 'output_lang': 250008,\n",
       " 'index': 0,\n",
       " 'dataset_index': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e74c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204e23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl  = DataLoader(md, batch_size = 2, shuffle=True, collate_fn=MBARTQGDataLoaderCollator(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc16a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[250008,   9020,   1556, 112730,     10,  88551,  31486,     23,     70,\n",
       "            1346,  53099,      4,    136,   1556,  13036,  12275,      7,     99,\n",
       "           54511,   2069,    136,     10,   5361,  93425,    136,   1305,      9,\n",
       "           57877,   1830,  26908,  11698,     71,     23,   1061,    497,      5,\n",
       "           27985,    214,  48962,   3847,      7,   1556,   6863,  10336, 167565,\n",
       "            1314,     23,   9020,      5,   9020,   1556,   2809,     70,  53550,\n",
       "             100,  54180,  26719,  70189,    384,  16196,     18,   5173,  72208,\n",
       "               4,  38213,  16635,    429,   5173,  62634,      4,   7948,   4378,\n",
       "            5173,  71204,      4,    581,  21027, 100974,  55283,      7,     15,\n",
       "          160733,    247,   2646,  34997,  40073,   5173,  88235,      4,  23213,\n",
       "           11584,   5128,      7,   5173,  88235,      4,    335,  17336,   2037,\n",
       "            5173,  84308,      4,    581,  14407,  18621,  41626, 223265,      4,\n",
       "             438,  19514,  37639, 146783,      4,  11954, 192728, 119845,      4,\n",
       "             310,   1326,  52522,  16883, 117593,      4,  74412,   1151,     53,\n",
       "             717,   4028,     12,    581,  42506,     19, 144898,   6619,  22964,\n",
       "             126,  15130,  94834,    136,    581,  18813,     25,      7, 218532,\n",
       "           71189,      5,    438,  22819,  39329,      7,    136,   1346, 114014,\n",
       "            1295,   9020,  26698,     74,  71305,   4841,  42454,      4,  74733,\n",
       "           43482,    206, 124324,      4,  11617,   2041,   1212,      4,  75526,\n",
       "           12969,    982,      4, 135730,  25074,    669,      4, 114411,    438,\n",
       "            1414,      4,  78182,  36293,      4,   8352,  39391,     53,      4,\n",
       "             341,  11906, 114996,   4293,    136,  11555,  10013,      9,   5267,\n",
       "           16196,      5,   1301,    111,   2021,   1065, 117008,    268,      4,\n",
       "              70,  56101,  62903,   3847,  54530,    765,  39958,   3687,     99,\n",
       "            4426,   8266,   2740,   2347,  25674,  16556,  13038,  16093,   8266,\n",
       "            2740,      5,   9020,     83,     10,  13036,  11698,    100, 113976,\n",
       "           36049,      4,    678,  12275,      7,  26719,   4831, 226462,  18222,\n",
       "               4,    581,   9572,  22278,     73, 137869,    136,    581,   9020,\n",
       "          137869,      5,  52455, 113976,  26693,      7,    765,   2809,   5423,\n",
       "              23,   9020,      4,  26719,     70,   5700, 113976,    221,   2631,\n",
       "            6264,  24453,   7768,  14202,      4, 226347,    390,     70,   4831,\n",
       "           16792,  20231,      5,      2],\n",
       "         [250008,    581,   5117,  36137,      9,   1264,    669,   2517,     23,\n",
       "          109261,  16037,    111,  10776,      7,  43573, 149397,  49086,     11,\n",
       "              15,   3464,      7,  39501,    297, 127067,    111,     70, 149397,\n",
       "           49086,   1657,    814, 110343,     16,   3542,  56101,    136,  15672,\n",
       "           28704,   1314,   2750,  11814,     47,  47445,     10,   6602,    707,\n",
       "            1286,     98,  25134,  69919,      4,   1295,    729,  15276,     98,\n",
       "           19364,      5, 133698,     70,    148,   4200,    214,   1615,      4,\n",
       "            3129,     21,  14437,  24189,  36236,      4,     70,  43904,    111,\n",
       "             450,  90695,  10861,    297,   1295,    645,  75534,     23,     70,\n",
       "           51065,     15,   5465, 131554,     23,   3060,   5369,     16,     47,\n",
       "            3060,   1781,     23,     70,  41710,      5,  17006,    111,     70,\n",
       "             148,   4200,   1314,   3542,   4426,   8266,   2740,  50645,   1177,\n",
       "           34958,  16093,   8266,   2740,      4,    678,    142, 118055, 123875,\n",
       "             111,  21689,   5245,      5,    581,   9615,  19929,      7,  99201,\n",
       "          101314,  28882,   1098,      4,    636,   8962,   1950,  38648,      4,\n",
       "           18813,  38157,  41097,      4,  77185,   7432,      4,  20387,   5342,\n",
       "               4,  54041, 140437,   1950,  38648,      4,  55609,   1950,  38648,\n",
       "             136, 130026,  28952,      5,  30195,      7,    136,   3789,  22962,\n",
       "           93324,      7,    111,     70,    148,   4200,    214,  29398,      7,\n",
       "           27983, 158930,  25842,    678,   2363,  87143,      5,   2022,   4021,\n",
       "            2856,    509,     70,  14037,     56,    111, 101314,  28882,   1098,\n",
       "               4, 154564,  25075,  33263, 135442,      4,     10, 197097, 103993,\n",
       "             148,   4200,     56,    136,  88898,     42,   2750,      4,  33233,\n",
       "             678,   1919,  14449,      4,  30666,    297,  56101, 130367,  16070,\n",
       "              23,  64972,      5,      2,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[250008,    360,   2367,      6, 104241,    621,     70,  56101,  62903,\n",
       "            1346,  70318,      7,   8121,     71,     32,      2],\n",
       "         [250008,   4865,    509,     70,  15889,   2481,    111,   2684,    148,\n",
       "            4200,   1314,     32,      2,      1,      1,      1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9480de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from src.model.mbart_qg import MBARTQG, MBARTQGDataLoaderCollator\n",
    "from src.eval_utils.evaluate_utils import HFMetric, MultiHFMetric\n",
    "\n",
    "import spacy\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "class SpacyTokenizer():\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    def __call__(self, x):\n",
    "        return [t.text for t in self.nlp.tokenizer(x)]\n",
    "st = SpacyTokenizer()\n",
    "validation_metrics = MultiHFMetric(\n",
    "    sacrebleu = HFMetric('sacrebleu', lambda x : x['score'], tokenize = 'intl'),\n",
    "    rouge = HFMetric('rouge', lambda x : x['rougeL'], tokenizer = st)\n",
    ")\n",
    "\n",
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'\n",
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "train_datasets_name = [\"squad-en-en.pb.json\",\"fquad-fr-fr.pb.json\"]\n",
    "valid_datasets_name = [\"fquad-fr-fr.pb.json\"]\n",
    "train_datasets = {}\n",
    "valid_datasets = {}\n",
    "\n",
    "for dataset_name in train_datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        il, ol = dataset_name.split('.')[0].split('-')[-2], dataset_name.split('.')[0].split('-')[-1]\n",
    "        data = json.load(f)\n",
    "        train_datasets[dataset_name.split('.')[0]] = FQAGPBDataset(\n",
    "            data[\"train\"],\n",
    "            sampler = lambda x : [x[random.randint(0, len(x) - 1)]],\n",
    "            input_lang = il, output_lang = ol\n",
    "        )\n",
    "for dataset_name in valid_datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        il, ol = dataset_name.split('.')[0].split('-')[-2], dataset_name.split('.')[0].split('-')[-1]\n",
    "        data = json.load(f)\n",
    "        valid_datasets[dataset_name.split('.')[0]] = FQAGPBDataset(\n",
    "            data[\"valid\"],\n",
    "            sampler = lambda x : [x[random.randint(0, len(x) - 1)]],\n",
    "            input_lang = il, output_lang = ol\n",
    "        )\n",
    "\n",
    "model = MBARTQG(\n",
    "    pretrained_name = \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    fixed_encoder = True,\n",
    "    validation_callback = validation_metrics, log_dir = os.path.join(os.path.expandvars(\"$QA_LOG\"), 'test')\n",
    ")\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=os.path.expandvars(\"$QA_LOG\"), name=\"test\")\n",
    "tb_logger.log_hyperparams({\"test\": 1 })\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "train_dl  = DataLoader(KeyMapDataset(MixedDataset(*train_datasets.values())), batch_size = 2, shuffle=True, num_workers=8, collate_fn=MBARTQGDataLoaderCollator(model.tokenizer))\n",
    "valid_dl  = DataLoader(KeyMapDataset(MixedDataset(*valid_datasets.values())), batch_size = 2, shuffle=False, num_workers=8, collate_fn=MBARTQGDataLoaderCollator(model.tokenizer))\n",
    "\n",
    "\n",
    "checkpoint_callback_val_loss = ModelCheckpoint(monitor='val/loss', save_top_k=2, mode=\"min\", filename=\"val-loss-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "checkpoint_callback_val_sacrebleu = ModelCheckpoint(monitor='val/sacrebleu', save_top_k=2, mode=\"max\", filename=\"val-sacrebleu-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "checkpoint_callback_val_rouge = ModelCheckpoint(monitor='val/rouge', save_top_k=2, mode=\"max\", filename=\"val-rouge-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "\n",
    "callbacks = [\n",
    "    lr_monitor,\n",
    "    checkpoint_callback_val_loss,\n",
    "    checkpoint_callback_val_rouge,\n",
    "    checkpoint_callback_val_sacrebleu\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78133a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/people/gerald/lib/miniconda3/envs/annotationbob/lib/python3.8/site-packages/torch/optim/sgd.py:105: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(SGD, self).__init__(params, defaults)\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | MBartForConditionalGeneration | 610 M \n",
      "--------------------------------------------------------\n",
      "610 M     Trainable params\n",
      "0         Non-trainable params\n",
      "610 M     Total params\n",
      "2,443.522 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/gerald/lib/miniconda3/envs/annotationbob/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c287c8fdca4295a0833d3d280648b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=tb_logger, \n",
    "    log_every_n_steps=1, \n",
    "    callbacks=callbacks, \n",
    "    enable_progress_bar=True,\n",
    "    limit_train_batches=10000, \n",
    "    max_epochs=250, \n",
    "    accumulate_grad_batches=64,\n",
    "    accelerator='gpu',\n",
    "    devices=[1]\n",
    ")\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff7d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
