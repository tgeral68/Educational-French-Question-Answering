{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00539d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/people/gerald/Documents/repositories/Educational-French-Question-Answering\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd94971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import evaluate\n",
    "import random\n",
    "from src.data_utils.pb_corpus import FQAGPBDataset\n",
    "from src.data_utils.corpus import MixedDataset, KeyMapDataset\n",
    "from src.model.mbart_qg import MBARTQGDataLoaderCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3753bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'\n",
    "os.environ['QA_LOG'] = \"/data/workdir/gerald/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee4ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "datasets_name = [\"squad-en-en.pb.json\",\"fquad-fr-fr.pb.json\"]\n",
    "datasets = {}\n",
    "\n",
    "split = \"train\"\n",
    "\n",
    "for dataset_name in datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        data = json.load(f)\n",
    "        datasets[dataset_name.split('.')[0]] = FQAGPBDataset(data['train'], sampler = lambda x : [x[random.randint(0, len(x) - 1)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda9181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = KeyMapDataset(MixedDataset(*datasets.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc5baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_default': True,\n",
       " 'id': 0,\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to <hl>Saint Bernadette Soubirous<hl> in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question_type': 'NONE',\n",
       " 'input_lang': 250008,\n",
       " 'output_lang': 250008,\n",
       " 'index': 0,\n",
       " 'dataset_index': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "948dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b960d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl  = DataLoader(md, batch_size = 2, shuffle=True, collate_fn=MBARTQGDataLoaderCollator(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae49afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[250008,    239,  45556,  53495,    217,    427,    201,   1837,      6,\n",
       "           58246,      7,     22,  20938,   1734,  57559,      7,    104,     25,\n",
       "           70353,    569,  53495,  18750,     95,   1001,    104,     25,  58246,\n",
       "               7,  61570,     95,  97879,      4,    199,   6181,      7,     82,\n",
       "             199,  11374,      4,   1609,   5896,      4,    418,   1745,    115,\n",
       "            3622,      4,     96,     25, 137858,  62195,      4,     96,     25,\n",
       "          100023,      4,     21,  38397,     82,     96,     25,  44713,  11129,\n",
       "               4,  48877,    418,   1745,      4,     82,     96,     25,  74896,\n",
       "               4,   4426,   8266,   2740,   2947,      4,    363,   1745,  16093,\n",
       "            8266,   2740,      5,  20097,   4360,  30686,      4,    382,    305,\n",
       "            2489,      6, 126907,      7,   3537,  40279,     18,    807,     21,\n",
       "           45556,    113, 107056,   5460,      6,  58246,      5,    339,     26,\n",
       "           69442,  11685,      8, 202104,    104,     25,  58246,  14283,      8,\n",
       "           66566,      4,    304,   1745,      4,    405,    569, 101717,     13,\n",
       "              41,     21,  20288,    104,     25,  58246,   5773,  42332,  14951,\n",
       "           59084,      6,  58246,    578,  19249, 168109,      6, 126907,      5,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [250008,  77168,    214,     10, 114122,   2412,  76104,    297,  87420,\n",
       "             604,  64685,   8770,  44462,      4,  31485,  66933,     70,  40111,\n",
       "             111,  33418,     99,   2027,   3422,     86,  13038,     98,     70,\n",
       "            2071,    133,    111,    601,  22553,      5,  80658,   4665,   1814,\n",
       "              39,     23,    604,   6049,      7,   1902,  79516,     71,    604,\n",
       "              21,    282,      4,    136,    604,  46223,  22553,    509,  76746,\n",
       "             297,    390,  60199,  20251,    933,      5, 194397,  39395,  18982,\n",
       "               4,   2412,  29131,     44,   1177,    344,    136,     51,  19256,\n",
       "             830,    136,    390,   4122,      9,  67884,   2240,     53,   2412,\n",
       "             509,     44,  92831,   4861,    153,     48,  64807,      4,    378,\n",
       "            2940,    268,  55681,   5281,    740,   4687,     68,     71,     98,\n",
       "            4426,   8266,   2740,  15665,     90,   5636,      4,   1039,  18982,\n",
       "           91255,  16093,   8266,   2740,      4,     99,  23552,  11015,  37195,\n",
       "              23,     70, 105216,      4,     99,     70,  32070,    111,  16503,\n",
       "               5,   1840,    775,    136,  39457,     42,  18813,  38157,  25714,\n",
       "               4,    136,    604,     88,  18557,  69190,    191,      4,  31678,\n",
       "              56,    748,  96513,   1995,    111, 102126,      4,   3542,     99,\n",
       "             604,  47219,  13482,      5,   1840, 185839,   8093,    663,  27160,\n",
       "           72173,      4,   4252,    416,      4,    509,     21,    532,  54799,\n",
       "             604,  47219,  13482,    237,     10,   4568,  50336,      5,      2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[250008, 106935,     33,    104,     25,  58246,      7,   2045, 192226,\n",
       "               7,    253,     96,     25,  74896,    253,  36166,    133,  17346,\n",
       "             705,      2],\n",
       "         [250008,    601,    528,     33,   6777,  44109,  31485,     68,     32,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from src.model.mbart_qg import MBARTQG, MBARTQGDataLoaderCollator\n",
    "from src.eval_utils.evaluate_utils import HFMetric, MultiHFMetric\n",
    "\n",
    "import spacy\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "class SpacyTokenizer():\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    def __call__(self, x):\n",
    "        return [t.text for t in self.nlp.tokenizer(x)]\n",
    "st = SpacyTokenizer()\n",
    "validation_metrics = MultiHFMetric(\n",
    "    sacrebleu = HFMetric('sacrebleu', lambda x : x['score'], tokenize = 'intl'),\n",
    "    rouge = HFMetric('rouge', lambda x : x['rougeL'], tokenizer = st)\n",
    ")\n",
    "\n",
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'\n",
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "train_datasets_name = [\"squad-en-en.pb.json\",\"fquad-fr-fr.pb.json\"]\n",
    "valid_datasets_name = [\"fquad-fr-fr.pb.json\"]\n",
    "train_datasets = {}\n",
    "valid_datasets = {}\n",
    "\n",
    "for dataset_name in train_datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        il, ol = dataset_name.split('.')[0].split('-')[-2], dataset_name.split('.')[0].split('-')[-1]\n",
    "        data = json.load(f)\n",
    "        train_datasets[dataset_name.split('.')[0]] = FQAGPBDataset(\n",
    "            data[\"train\"],\n",
    "            sampler = lambda x : [x[random.randint(0, len(x) - 1)]],\n",
    "            input_lang = il, output_lang = ol\n",
    "        )\n",
    "for dataset_name in valid_datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        il, ol = dataset_name.split('.')[0].split('-')[-2], dataset_name.split('.')[0].split('-')[-1]\n",
    "        data = json.load(f)\n",
    "        valid_datasets[dataset_name.split('.')[0]] = FQAGPBDataset(\n",
    "            data[\"valid\"],\n",
    "            sampler = lambda x : [x[random.randint(0, len(x) - 1)]],\n",
    "            input_lang = il, output_lang = ol\n",
    "        )\n",
    "\n",
    "model = MBARTQG(\n",
    "    pretrained_name = \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    fixed_encoder = True,\n",
    "    validation_callback = validation_metrics, log_dir = os.path.join(os.path.expandvars(\"$QA_LOG\"), 'test')\n",
    ")\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=os.path.expandvars(\"$QA_LOG\"), name=\"test\")\n",
    "tb_logger.log_hyperparams({\"test\": 1 })\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "train_dl  = DataLoader(KeyMapDataset(MixedDataset(*train_datasets.values())), batch_size = 2, shuffle=True, num_workers=8, collate_fn=MBARTQGDataLoaderCollator(model.tokenizer))\n",
    "valid_dl  = DataLoader(KeyMapDataset(MixedDataset(*valid_datasets.values())), batch_size = 2, shuffle=False, num_workers=8, collate_fn=MBARTQGDataLoaderCollator(model.tokenizer))\n",
    "\n",
    "\n",
    "checkpoint_callback_val_loss = ModelCheckpoint(monitor='val/loss', save_top_k=2, mode=\"min\", filename=\"val-loss-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "checkpoint_callback_val_sacrebleu = ModelCheckpoint(monitor='val/sacrebleu', save_top_k=2, mode=\"max\", filename=\"val-sacrebleu-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "checkpoint_callback_val_rouge = ModelCheckpoint(monitor='val/rouge', save_top_k=2, mode=\"max\", filename=\"val-rouge-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "\n",
    "callbacks = [\n",
    "    lr_monitor,\n",
    "    checkpoint_callback_val_loss,\n",
    "    checkpoint_callback_val_rouge,\n",
    "    checkpoint_callback_val_sacrebleu\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=tb_logger, \n",
    "    log_every_n_steps=1, \n",
    "    callbacks=callbacks, \n",
    "    enable_progress_bar=True,\n",
    "    limit_train_batches=10000, \n",
    "    max_epochs=250, \n",
    "    resume_from_checkpoint=\"/data/workdir/gerald/log/test/version_9/checkpoints/val-sacrebleu-checkpoint-epoch=23-val_loss=0.00.ckpt\",\n",
    "    accumulate_grad_batches=64,\n",
    "    accelerator='gpu',\n",
    "    devices=[1]\n",
    ")\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e636f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
