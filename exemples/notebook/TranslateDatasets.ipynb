{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c497ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/people/gerald/Documents/repositories/Educational-French-Question-Answering\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaddf9",
   "metadata": {},
   "source": [
    "## Translate corpus output\n",
    "The objective of this notebook is to produce datasets with output (questions) in different english while context being in french and reversly. To this end we leverage translation model MBart on the questions of the originals datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a675126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from src.data_utils import corpus, pb_corpus\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6de933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3439962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "datasets_name = [\"squad.pb.json\",\"fquad.pb.json\", \"piaf.pb.json\", \"freelance.pb.json\", \"isahit.pb.json\"]\n",
    "datasets = {}\n",
    "\n",
    "for dataset_name in datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        data = json.load(f)\n",
    "        if('test' not in data):\n",
    "            datasets[dataset_name.split('.')[0]] = {'test': data}\n",
    "        else:\n",
    "            datasets[dataset_name.split('.')[0]] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b1c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MAP = {\n",
    "    \"fr\" : \"fr_XX\",\n",
    "    \"en\" : \"en_XX\",\n",
    "    \"es\" : \"es_XX\",\n",
    "    \"ja\" : \"ja_XX\",\n",
    "    \"pt\" : \"pt_XX\",\n",
    "    \"tl\" : \"tl_XX\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3fedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name=\"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "load_lang=[\"fr\", \"en\"]\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801a298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 0/3 [00:00<?, ?it/s]/people/gerald/lib/miniconda3/envs/annotationbob/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:21<00:00, 67.32s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:39<00:00, 13.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:12<00:00, 64.25s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.50s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.94s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "device = 'cuda:0'\n",
    "model.eval()\n",
    "model.to(device)\n",
    "dataset_translated = {}\n",
    "\n",
    "for dataset_name, dataset_data in datasets.items():\n",
    "    input_lang = \"fr\"\n",
    "    output_lang = \"en\"\n",
    "    if('squad' in dataset_name):\n",
    "        input_lang = \"en\"\n",
    "        output_lang = \"fr\"\n",
    "    dataset_translated[dataset_name] = dataset_data\n",
    "\n",
    "    tokenizer.src_lang = LANGUAGE_MAP[input_lang]\n",
    "    for dataset_split_name, dataset_split_data in tqdm.tqdm(dataset_data.items()):\n",
    "        for i, document in enumerate(dataset_split_data):\n",
    "            for j, qa in enumerate(document['qas']):\n",
    "                input_data = tokenizer([qa['question']], return_tensors='pt')\n",
    "                output_data = model.generate(**{k: v.to(device) for k, v in input_data.items()}, \n",
    "                                             forced_bos_token_id=tokenizer.lang_code_to_id[LANGUAGE_MAP[output_lang]])\n",
    "                output_text = tokenizer.batch_decode(output_data, skip_special_tokens=True)\n",
    "                dataset_translated[dataset_name][dataset_split_name][i]['qas'][j]['question'] = output_text[0]\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea1d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  5.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 48.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_data in dataset_translated.items():\n",
    "    input_lang = \"fr\"\n",
    "    output_lang = \"en\"\n",
    "    if('squad' in dataset_name):\n",
    "        input_lang = \"en\"\n",
    "        output_lang = \"fr\"\n",
    "    for dataset_split_name, dataset_split_data in tqdm.tqdm(dataset_data.items()):\n",
    "        dataset_folder = os.path.join(data_folder, \n",
    "                               dataset_name + \"-\" + \n",
    "                                      input_lang + \"-\" +\n",
    "                                      output_lang)\n",
    "        os.makedirs(dataset_folder, exist_ok=True)\n",
    "        with open(os.path.join(dataset_folder, \n",
    "                               dataset_split_name), 'w') as f :\n",
    "            json.dump(dataset_split_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86c9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
