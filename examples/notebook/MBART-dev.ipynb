{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13850eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/people/gerald/Documents/repositories/Educational-French-Question-Answering\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd94971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import evaluate\n",
    "import random\n",
    "from src.data_utils.pb_corpus import FQAGPBDataset\n",
    "from src.data_utils.corpus import MixedDataset, KeyMapDataset\n",
    "from src.model.mbart_qg import MBARTQGDataLoaderCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64146869",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'\n",
    "os.environ['QA_LOG'] = \"/data/workdir/gerald/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee4ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "datasets_name = [\"squad-en-en.pb.json\",\"fquad-fr-fr.pb.json\"]\n",
    "datasets = {}\n",
    "\n",
    "split = \"train\"\n",
    "\n",
    "for dataset_name in datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        data = json.load(f)\n",
    "        datasets[dataset_name.split('.')[0]] = FQAGPBDataset(data['train'], sampler = lambda x : [x[random.randint(0, len(x) - 1)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda9181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = KeyMapDataset(MixedDataset(*datasets.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50cf199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_default': True,\n",
       " 'id': 0,\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to <hl>Saint Bernadette Soubirous<hl> in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question_type': 'NONE',\n",
       " 'input_lang': 250008,\n",
       " 'output_lang': 250008,\n",
       " 'index': 0,\n",
       " 'dataset_index': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858a6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008ba4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl  = DataLoader(md, batch_size = 2, shuffle=True, collate_fn=MBARTQGDataLoaderCollator(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24a59f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[250008,    239,  45556,  53495,    217,    427,    201,   1837,      6,\n",
       "           58246,      7,     22,  20938,   1734,  57559,      7,    104,     25,\n",
       "           70353,    569,  53495,  18750,     95,   1001,    104,     25,  58246,\n",
       "               7,  61570,     95,  97879,      4,    199,   6181,      7,     82,\n",
       "             199,  11374,      4,   1609,   5896,      4,    418,   1745,    115,\n",
       "            3622,      4,     96,     25, 137858,  62195,      4,     96,     25,\n",
       "          100023,      4,     21,  38397,     82,     96,     25,  44713,  11129,\n",
       "               4,  48877,    418,   1745,      4,     82,     96,     25,  74896,\n",
       "               4,   4426,   8266,   2740,   2947,      4,    363,   1745,  16093,\n",
       "            8266,   2740,      5,  20097,   4360,  30686,      4,    382,    305,\n",
       "            2489,      6, 126907,      7,   3537,  40279,     18,    807,     21,\n",
       "           45556,    113, 107056,   5460,      6,  58246,      5,    339,     26,\n",
       "           69442,  11685,      8, 202104,    104,     25,  58246,  14283,      8,\n",
       "           66566,      4,    304,   1745,      4,    405,    569, 101717,     13,\n",
       "              41,     21,  20288,    104,     25,  58246,   5773,  42332,  14951,\n",
       "           59084,      6,  58246,    578,  19249, 168109,      6, 126907,      5,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [250008,  77168,    214,     10, 114122,   2412,  76104,    297,  87420,\n",
       "             604,  64685,   8770,  44462,      4,  31485,  66933,     70,  40111,\n",
       "             111,  33418,     99,   2027,   3422,     86,  13038,     98,     70,\n",
       "            2071,    133,    111,    601,  22553,      5,  80658,   4665,   1814,\n",
       "              39,     23,    604,   6049,      7,   1902,  79516,     71,    604,\n",
       "              21,    282,      4,    136,    604,  46223,  22553,    509,  76746,\n",
       "             297,    390,  60199,  20251,    933,      5, 194397,  39395,  18982,\n",
       "               4,   2412,  29131,     44,   1177,    344,    136,     51,  19256,\n",
       "             830,    136,    390,   4122,      9,  67884,   2240,     53,   2412,\n",
       "             509,     44,  92831,   4861,    153,     48,  64807,      4,    378,\n",
       "            2940,    268,  55681,   5281,    740,   4687,     68,     71,     98,\n",
       "            4426,   8266,   2740,  15665,     90,   5636,      4,   1039,  18982,\n",
       "           91255,  16093,   8266,   2740,      4,     99,  23552,  11015,  37195,\n",
       "              23,     70, 105216,      4,     99,     70,  32070,    111,  16503,\n",
       "               5,   1840,    775,    136,  39457,     42,  18813,  38157,  25714,\n",
       "               4,    136,    604,     88,  18557,  69190,    191,      4,  31678,\n",
       "              56,    748,  96513,   1995,    111, 102126,      4,   3542,     99,\n",
       "             604,  47219,  13482,      5,   1840, 185839,   8093,    663,  27160,\n",
       "           72173,      4,   4252,    416,      4,    509,     21,    532,  54799,\n",
       "             604,  47219,  13482,    237,     10,   4568,  50336,      5,      2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[250008, 106935,     33,    104,     25,  58246,      7,   2045, 192226,\n",
       "               7,    253,     96,     25,  74896,    253,  36166,    133,  17346,\n",
       "             705,      2],\n",
       "         [250008,    601,    528,     33,   6777,  44109,  31485,     68,     32,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e385225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from src.model.mbart_qg import MBARTQG, MBARTQGDataLoaderCollator\n",
    "from src.eval_utils.evaluate_utils import HFMetric, MultiHFMetric\n",
    "\n",
    "import spacy\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "class SpacyTokenizer():\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    def __call__(self, x):\n",
    "        return [t.text for t in self.nlp.tokenizer(x)]\n",
    "st = SpacyTokenizer()\n",
    "validation_metrics = MultiHFMetric(\n",
    "    sacrebleu = HFMetric('sacrebleu', lambda x : x['score'], tokenize = 'intl'),\n",
    "    rouge = HFMetric('rouge', lambda x : x['rougeL'], tokenizer = st)\n",
    ")\n",
    "\n",
    "os.environ['EFQADATA'] = '/people/gerald/Documents/repositories/Educational-French-Question-Answering/dataset'\n",
    "data_folder = os.path.expandvars(\"$EFQADATA/source\")\n",
    "train_datasets_name = [\"squad-en-en.pb.json\",\"fquad-fr-fr.pb.json\"]\n",
    "valid_datasets_name = [\"fquad-fr-fr.pb.json\"]\n",
    "train_datasets = {}\n",
    "valid_datasets = {}\n",
    "\n",
    "for dataset_name in train_datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        il, ol = dataset_name.split('.')[0].split('-')[-2], dataset_name.split('.')[0].split('-')[-1]\n",
    "        data = json.load(f)\n",
    "        train_datasets[dataset_name.split('.')[0]] = FQAGPBDataset(\n",
    "            data[\"train\"],\n",
    "            sampler = lambda x : [x[random.randint(0, len(x) - 1)]],\n",
    "            input_lang = il, output_lang = ol\n",
    "        )\n",
    "for dataset_name in valid_datasets_name: \n",
    "    with open(os.path.join(data_folder, dataset_name)) as f:\n",
    "        il, ol = dataset_name.split('.')[0].split('-')[-2], dataset_name.split('.')[0].split('-')[-1]\n",
    "        data = json.load(f)\n",
    "        valid_datasets[dataset_name.split('.')[0]] = FQAGPBDataset(\n",
    "            data[\"valid\"],\n",
    "            sampler = lambda x : [x[random.randint(0, len(x) - 1)]],\n",
    "            input_lang = il, output_lang = ol\n",
    "        )\n",
    "\n",
    "model = MBARTQG(\n",
    "    pretrained_name = \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    fixed_encoder = True,\n",
    "    validation_callback = validation_metrics, log_dir = os.path.join(os.path.expandvars(\"$QA_LOG\"), 'test')\n",
    ")\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=os.path.expandvars(\"$QA_LOG\"), name=\"test\")\n",
    "tb_logger.log_hyperparams({\"test\": 1 })\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "train_dl  = DataLoader(KeyMapDataset(MixedDataset(*train_datasets.values())), batch_size = 2, shuffle=True, num_workers=8, collate_fn=MBARTQGDataLoaderCollator(model.tokenizer))\n",
    "valid_dl  = DataLoader(KeyMapDataset(MixedDataset(*valid_datasets.values())), batch_size = 2, shuffle=False, num_workers=8, collate_fn=MBARTQGDataLoaderCollator(model.tokenizer))\n",
    "\n",
    "\n",
    "checkpoint_callback_val_loss = ModelCheckpoint(monitor='val/loss', save_top_k=2, mode=\"min\", filename=\"val-loss-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "checkpoint_callback_val_sacrebleu = ModelCheckpoint(monitor='val/sacrebleu', save_top_k=2, mode=\"max\", filename=\"val-sacrebleu-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "checkpoint_callback_val_rouge = ModelCheckpoint(monitor='val/rouge', save_top_k=2, mode=\"max\", filename=\"val-rouge-checkpoint-{epoch:02d}-{val_loss:.2f}\")\n",
    "\n",
    "callbacks = [\n",
    "    lr_monitor,\n",
    "    checkpoint_callback_val_loss,\n",
    "    checkpoint_callback_val_rouge,\n",
    "    checkpoint_callback_val_sacrebleu\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca84fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /data/workdir/gerald/log/test/version_9/checkpoints/val-sacrebleu-checkpoint-epoch=23-val_loss=0.00.ckpt\n",
      "/people/gerald/lib/miniconda3/envs/annotationbob/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:345: UserWarning: The dirpath has changed from '/data/workdir/gerald/log/test/version_9/checkpoints' to '/data/workdir/gerald/log/test/version_11/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/people/gerald/lib/miniconda3/envs/annotationbob/lib/python3.8/site-packages/torch/optim/sgd.py:105: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(SGD, self).__init__(params, defaults)\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | MBartForConditionalGeneration | 610 M \n",
      "--------------------------------------------------------\n",
      "610 M     Trainable params\n",
      "0         Non-trainable params\n",
      "610 M     Total params\n",
      "2,443.522 Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at /data/workdir/gerald/log/test/version_9/checkpoints/val-sacrebleu-checkpoint-epoch=23-val_loss=0.00.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/gerald/lib/miniconda3/envs/annotationbob/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4271f19a2d43c1972ca95b2c4b4713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 10000it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=tb_logger, \n",
    "    log_every_n_steps=1, \n",
    "    callbacks=callbacks, \n",
    "    enable_progress_bar=True,\n",
    "    limit_train_batches=10000, \n",
    "    max_epochs=250, \n",
    "    resume_from_checkpoint=\"/data/workdir/gerald/log/test/version_9/checkpoints/val-sacrebleu-checkpoint-epoch=23-val_loss=0.00.ckpt\",\n",
    "    accumulate_grad_batches=64,\n",
    "    accelerator='gpu',\n",
    "    devices=[1]\n",
    ")\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
